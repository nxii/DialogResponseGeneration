{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/collection/ka2khan/thesis/Cond_Text_Gen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5566/5566 [22:07<00:00,  4.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses generated using at least one token: 99.71\n",
      "Avg. token usage: 4.762738052461373\n",
      "Experiment: DailyDialog\n",
      "Avg Recall Bleu: 0.09, Avg Precision Bleu: 0.05\n",
      "TTR: 1.1042088165731057\n",
      "ASL: 13.05298239310097\n",
      "PPL: 127.06401101180019\n",
      "Short responses count 0 words: 0, 1 words: 0\n",
      "Intra Dist-1: 0.82, Intra Dist-2: 0.98, Inter Dist-1: 0.52, Inter Dist-2: 0.89\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5046/5046 [20:12<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses generated using at least one token: 99.92\n",
      "Avg. token usage: 5.184839476813318\n",
      "Experiment: SWDA\n",
      "Avg Recall Bleu: 0.08, Avg Precision Bleu: 0.02\n",
      "TTR: 0.45499198593661133\n",
      "ASL: 10.904558065794689\n",
      "PPL: 183.07041754111867\n",
      "Short responses count 0 words: 0, 1 words: 0\n",
      "Intra Dist-1: 0.74, Intra Dist-2: 0.91, Inter Dist-1: 0.48, Inter Dist-2: 0.84\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5566/5566 [22:13<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: T5-DailyDialog\n",
      "Avg Recall Bleu: 0.08, Avg Precision Bleu: 0.04\n",
      "TTR: 7.660672400312745\n",
      "ASL: 11.499730506647502\n",
      "PPL: 436.09244729570787\n",
      "Short responses count 0 words: 0, 1 words: 42\n",
      "Intra Dist-1: 0.84, Intra Dist-2: 0.99, Inter Dist-1: 0.50, Inter Dist-2: 0.84\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5046/5046 [19:52<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: T5-SWDA\n",
      "Avg Recall Bleu: 0.07, Avg Precision Bleu: 0.02\n",
      "TTR: 7.316185076334886\n",
      "ASL: 7.548612762584225\n",
      "PPL: 2721.637323190426\n",
      "Short responses count 0 words: 21, 1 words: 13227\n",
      "Intra Dist-1: 0.57, Intra Dist-2: 0.98, Inter Dist-1: 0.52, Inter Dist-2: 0.89\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2Config, GPT2Tokenizer, GPT2LMHeadModel\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def compute_distinct(genereated_responses):\n",
    "    intra_dist_1 = []\n",
    "    intra_dist_2 = []\n",
    "    inter_dist_1 = []\n",
    "    inter_dist_2 = []\n",
    "    \n",
    "    for gen_resps in generated_responses:\n",
    "        inta_dist1 = []\n",
    "        inta_dist2 = []\n",
    "        \n",
    "        unigrams_all = Counter()\n",
    "        bigrams_all = Counter()\n",
    "        n_unigrams = 0\n",
    "        n_bigrams = 0\n",
    "        for gen_resp in gen_resps:\n",
    "            if len(gen_resp) == 0:\n",
    "                continue\n",
    "                \n",
    "            unigrams = Counter([tuple(gen_resp[i:i+1]) for i in range(len(gen_resp)-1)])\n",
    "            inta_dist1.append(len(unigrams)/len(gen_resp))\n",
    "            \n",
    "            if len(gen_resp) > 1:\n",
    "                bigrams = Counter([tuple(gen_resp[i:i+2]) for i in range(len(gen_resp)-1)])\n",
    "                inta_dist2.append(len(bigrams)/(len(gen_resp)-1))\n",
    "            \n",
    "            unigrams_all.update([tuple(gen_resp[i:i+1]) for i in range(len(gen_resp)-1)])\n",
    "            bigrams_all.update([tuple(gen_resp[i:i+2]) for i in range(len(gen_resp)-1)])\n",
    "            \n",
    "            n_unigrams += len(gen_resp) \n",
    "            if len(gen_resp) > 1:\n",
    "                n_bigrams += (len(gen_resp)-1)\n",
    "        \n",
    "        intra_dist_1.append(np.mean(inta_dist1))\n",
    "        intra_dist_2.append(np.mean(inta_dist2))\n",
    "        \n",
    "        inter_dist_1.append(len(unigrams_all)/n_unigrams)\n",
    "        \n",
    "        if n_bigrams > 0:\n",
    "            inter_dist_2.append(len(bigrams_all)/n_bigrams)\n",
    "    \n",
    "    return np.mean(intra_dist_1), np.mean(intra_dist_2), np.mean(inter_dist_1), np.mean(inter_dist_2)\n",
    "        \n",
    "\n",
    "def compute_PPL(model, tokenizer, generated_responses):\n",
    "    perplexity_scores = []\n",
    "    for gen_resps in tqdm(generated_responses):\n",
    "        for gen_resp in gen_resps:\n",
    "            gen_resp = ' '.join(gen_resp)\n",
    "            try:\n",
    "                input_ids = torch.tensor(tokenizer.encode(gen_resp)).unsqueeze(0) \n",
    "                input_ids = input_ids.cuda()\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(input_ids, labels=input_ids)\n",
    "                loss, logits = outputs[:2]\n",
    "                perplexity_scores.append(math.exp(loss))\n",
    "            except:\n",
    "                pass\n",
    "                #print(gen_resp)\n",
    "                #print('PPL computation error!')\n",
    "    \n",
    "    return np.mean(perplexity_scores)\n",
    "\n",
    "def compute_ASL(generated_responses):\n",
    "    asl = []\n",
    "    for gen_resps in generated_responses:\n",
    "        for resp in gen_resps:\n",
    "            asl.append(len(resp))\n",
    "    \n",
    "    return np.mean(asl)\n",
    "\n",
    "def compute_TTR(generated_responses):\n",
    "    all_tokens = []\n",
    "    for gen_resp in generated_responses:\n",
    "        all_tokens.extend(gen_resp)\n",
    "    \n",
    "    return len(set(all_tokens)) / len(all_tokens)\n",
    "\n",
    "def compute_bleu(generated_responses, ground_truth):\n",
    "    \n",
    "    recall_bleus = []\n",
    "    precision_bleus = []\n",
    "    for index in range(len(ground_truth)):\n",
    "        bleu_scores = []\n",
    "        for gen_resp in generated_responses[index]:\n",
    "            try:\n",
    "                bleu_scores.append(sentence_bleu([ground_truth[index]], gen_resp, smoothing_function=SmoothingFunction().method7, weights=[1./3, 1./3, 1./3]))\n",
    "            except:\n",
    "                bleu_scores.append(0.0)\n",
    "        \n",
    "        recall_bleus.append(np.max(bleu_scores))\n",
    "        precision_bleus.append(np.mean(bleu_scores))\n",
    "    \n",
    "    \n",
    "    return np.mean(recall_bleus), np.mean(precision_bleus)\n",
    "\n",
    "def count_short_responses(generated_responses):\n",
    "    counts = [0, 0]\n",
    "    for gen_resps in generated_responses:\n",
    "        for gen_resp in gen_resps:\n",
    "            if len(gen_resp) == 0:\n",
    "                counts[0] += 1\n",
    "            elif len(gen_resp) == 1:     \n",
    "                counts[1] += 1\n",
    "    \n",
    "    return counts\n",
    "\n",
    "def load_DialogWAE_results(f_obj):\n",
    "    output = []\n",
    "    o_dict = {'generated_sentences': []}\n",
    "    lines = f_obj.readlines()\n",
    "    for line in tqdm(lines):\n",
    "        #print(line)\n",
    "        if line.startswith('Target >>') == True:\n",
    "            line = line.rstrip('</s>')\n",
    "            o_dict['next sentence'] = line[line.index('>>')+2:].strip()\n",
    "        elif line.startswith('Sample') == True:     \n",
    "            line = line.rstrip('</s>')\n",
    "            o_dict['generated_sentences'].append(line[line.index('>>')+2:].strip())\n",
    "        elif line.strip() == '':\n",
    "            assert len(o_dict['generated_sentences']) == 10 and o_dict['next sentence'] != '', f'Error! {o_dict}'\n",
    "            output.append(o_dict)\n",
    "            o_dict = {'generated_sentences': []}\n",
    "    \n",
    "    return output\n",
    "\n",
    "def compute_token_usage(predicted_tokens, generated_responses):\n",
    "    \n",
    "    response_w_tokens = 0\n",
    "    tokens_found_counts = []\n",
    "    for q_index in range(len(predicted_tokens)):\n",
    "        for r_index in range(len(predicted_tokens[q_index])):\n",
    "            bow = predicted_tokens[q_index][r_index]\n",
    "            response = generated_responses[q_index][r_index]\n",
    "            tokens_found = 0\n",
    "            for token in bow:\n",
    "                if token in response:\n",
    "                    tokens_found += 1\n",
    "            \n",
    "            if tokens_found > 0:\n",
    "                response_w_tokens += 1\n",
    "                \n",
    "            tokens_found_counts.append(tokens_found)\n",
    "    \n",
    "    return (response_w_tokens / len(tokens_found_counts)) * 100, np.mean(tokens_found_counts)\n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "\n",
    "os.chdir('/collection/ka2khan/thesis/Cond_Text_Gen')\n",
    "print(os.getcwd())\n",
    "\n",
    "output_files = {\n",
    "    #'TACGAN End-to-End': 'outputs/DailyDialog_t5-large_gtk_w_keywords_TACGAN_EndtoEnd_output.json',\n",
    "    #'TACGAN wo ENC Dec Backprop': 'outputs/DailyDialog_TACGAN_wo_EncDec_Backprop_test_output_12.json',\n",
    "    #'T5 wo Keywords': 'outputs/DailyDialog_t5_wo_keywords_test_output_11.json',\n",
    "    #'DialogWAE': 'outputs/DailyDialog_DialogWAE_results.json',\n",
    "    #'S10': 'outputs/S10_output.json',\n",
    "    #'DialogWAE_DailyDialog': 'outputs/DialogWAE_DailyDialog.txt',\n",
    "    #'DialogWAE_SWDA': 'outputs/DialogWAE_SWDA.txt',\n",
    "    'DailyDialog': 'outputs/S6_DailyDialog.json',\n",
    "    'SWDA': 'outputs/S6_SWDA.json',\n",
    "    'T5-DailyDialog': 'outputs/T5_DailyDialog.json',\n",
    "    'T5-SWDA': 'outputs/T5_SWDA.json',\n",
    "    }\n",
    "\n",
    "for exp_name, file_path in output_files.items():\n",
    "    predicted_tokens = []\n",
    "    generated_responses = []\n",
    "    ground_truth = []\n",
    "    if exp_name == 'DialogWAE_DailyDialog' or exp_name == 'DialogWAE_SWDA':\n",
    "        f_obj = open(file_path)\n",
    "        output = load_DialogWAE_results(f_obj)\n",
    "        for item in tqdm(output):\n",
    "            ground_truth.append(word_tokenize(item['next sentence']))\n",
    "            \n",
    "            gen_resps = []\n",
    "            for index, gen_resp in enumerate(item['generated_sentences']):\n",
    "                gen_resps.append(word_tokenize(gen_resp))\n",
    "                \n",
    "            generated_responses.append(gen_resps)\n",
    "    else:\n",
    "        with open(file_path) as f_obj:\n",
    "            output = json.load(f_obj)\n",
    "        \n",
    "        for item in output:\n",
    "            predicted_tokens.append(item['predicted_sent_bow'])\n",
    "            ground_truth.append(item['next sentence'])\n",
    "            gen_resps = []\n",
    "            for index, gen_resp in enumerate(item['generated_sentences']):\n",
    "                #index = gen_resp.rfind('.')\n",
    "                #if index != -1:\n",
    "                #    en_resp = gen_resp[:index]\n",
    "                gen_resps.append(word_tokenize(gen_resp))\n",
    "                \n",
    "            generated_responses.append(gen_resps)\n",
    "\n",
    "\n",
    "    avg_recall_bleu, avg_precision_bleu = compute_bleu(generated_responses, ground_truth)\n",
    "    \n",
    "    first_responses = []\n",
    "    for gen_resps in generated_responses:\n",
    "        first_responses.append(gen_resps[0])\n",
    "\n",
    "    \n",
    "    assert len(generated_responses) == len(first_responses)\n",
    "        \n",
    "    ttr = compute_TTR(first_responses)\n",
    "    asl = compute_ASL(generated_responses)\n",
    "    \n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\n",
    "    config = GPT2Config.from_pretrained('gpt2-large')\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2-large', config=config)\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    \n",
    "    ppl = compute_PPL(model, tokenizer, generated_responses)\n",
    "    \n",
    "    if exp_name in ['DailyDialog', 'SWDA']:\n",
    "        token_percent, tokens_avg = compute_token_usage(predicted_tokens, generated_responses)\n",
    "        \n",
    "        print(f'Responses generated using at least one token: {token_percent:.2f}')\n",
    "        print(f'Avg. token usage: {tokens_avg}')\n",
    "    \n",
    "    counts = count_short_responses(generated_responses)\n",
    "    \n",
    "    intra_dist_1, intra_dist_2, inter_dist_1, inter_dist_2 = compute_distinct(generated_responses)\n",
    "    \n",
    "    print(f'Experiment: {exp_name}')\n",
    "    print(f'Avg Recall Bleu: {avg_recall_bleu:.2f}, Avg Precision Bleu: {avg_precision_bleu:.2f}')\n",
    "    print(f'TTR: {ttr * 100}')\n",
    "    print(f'ASL: {asl}')\n",
    "    print(f'PPL: {ppl}')\n",
    "    print(f'Short responses count 0 words: {counts[0]}, 1 words: {counts[1]}')\n",
    "    print(f'Intra Dist-1: {intra_dist_1:.2f}, Intra Dist-2: {intra_dist_2:.2f}, Inter Dist-1: {inter_dist_1:.2f}, Inter Dist-2: {inter_dist_2:.2f}')\n",
    "    \n",
    "    print()    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/collection/ka2khan/thesis/Cond_Text_Gen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44660/44660 [17:53<00:00, 41.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPL: 3457.175266843699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2Config, GPT2Tokenizer, GPT2LMHeadModel\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def compute_distinct(genereated_responses):\n",
    "    intra_dist_1 = []\n",
    "    intra_dist_2 = []\n",
    "    inter_dist_1 = []\n",
    "    inter_dist_2 = []\n",
    "    \n",
    "    for gen_resps in generated_responses:\n",
    "        inta_dist1 = []\n",
    "        inta_dist2 = []\n",
    "        \n",
    "        unigrams_all = Counter()\n",
    "        bigrams_all = Counter()\n",
    "        n_unigrams = 0\n",
    "        n_bigrams = 0\n",
    "        for gen_resp in gen_resps:\n",
    "            if len(gen_resp) == 0:\n",
    "                continue\n",
    "                \n",
    "            unigrams = Counter([tuple(gen_resp[i:i+1]) for i in range(len(gen_resp)-1)])\n",
    "            inta_dist1.append(len(unigrams)/len(gen_resp))\n",
    "            \n",
    "            if len(gen_resp) > 1:\n",
    "                bigrams = Counter([tuple(gen_resp[i:i+2]) for i in range(len(gen_resp)-1)])\n",
    "                inta_dist2.append(len(bigrams)/(len(gen_resp)-1))\n",
    "            \n",
    "            unigrams_all.update([tuple(gen_resp[i:i+1]) for i in range(len(gen_resp)-1)])\n",
    "            bigrams_all.update([tuple(gen_resp[i:i+2]) for i in range(len(gen_resp)-1)])\n",
    "            \n",
    "            n_unigrams += len(gen_resp) \n",
    "            if len(gen_resp) > 1:\n",
    "                n_bigrams += (len(gen_resp)-1)\n",
    "        \n",
    "        intra_dist_1.append(np.mean(inta_dist1))\n",
    "        intra_dist_2.append(np.mean(inta_dist2))\n",
    "        \n",
    "        inter_dist_1.append(len(unigrams_all)/n_unigrams)\n",
    "        \n",
    "        if n_bigrams > 0:\n",
    "            inter_dist_2.append(len(bigrams_all)/n_bigrams)\n",
    "    \n",
    "    return np.mean(intra_dist_1), np.mean(intra_dist_2), np.mean(inter_dist_1), np.mean(inter_dist_2)\n",
    "        \n",
    "\n",
    "def compute_PPL(model, tokenizer, generated_responses):\n",
    "    perplexity_scores = []\n",
    "    for gen_resps in tqdm(generated_responses):\n",
    "        for gen_resp in gen_resps:\n",
    "            gen_resp = ' '.join(gen_resp)\n",
    "            try:\n",
    "                input_ids = torch.tensor(tokenizer.encode(gen_resp)).unsqueeze(0) \n",
    "                input_ids = input_ids.cuda()\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(input_ids, labels=input_ids)\n",
    "                loss, logits = outputs[:2]\n",
    "                perplexity_scores.append(math.exp(loss))\n",
    "            except:\n",
    "                pass\n",
    "                #print(gen_resp)\n",
    "                #print('PPL computation error!')\n",
    "    \n",
    "    return np.mean(perplexity_scores)\n",
    "\n",
    "def compute_avg_PPL(model, tokenizer, generated_responses):\n",
    "    perplexity_scores = []\n",
    "    for gen_resp in tqdm(generated_responses):\n",
    "        try:\n",
    "            input_ids = torch.tensor(tokenizer.encode(gen_resp)).unsqueeze(0) \n",
    "            input_ids = input_ids.cuda()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, labels=input_ids)\n",
    "            loss, logits = outputs[:2]\n",
    "            perplexity_scores.append(math.exp(loss))\n",
    "        except:\n",
    "            pass\n",
    "            #print(gen_resp)\n",
    "            #print('PPL computation error!')\n",
    "    \n",
    "    return np.mean(perplexity_scores)\n",
    "\n",
    "def compute_ASL(generated_responses):\n",
    "    asl = []\n",
    "    for gen_resps in generated_responses:\n",
    "        for resp in gen_resps:\n",
    "            asl.append(len(resp))\n",
    "    \n",
    "    return np.mean(asl)\n",
    "\n",
    "def compute_TTR(generated_responses):\n",
    "    all_tokens = []\n",
    "    for gen_resp in generated_responses:\n",
    "        all_tokens.extend(gen_resp)\n",
    "    \n",
    "    return len(set(all_tokens)) / len(all_tokens)\n",
    "\n",
    "def compute_bleu(generated_responses, ground_truth):\n",
    "    \n",
    "    recall_bleus = []\n",
    "    precision_bleus = []\n",
    "    for index in range(len(ground_truth)):\n",
    "        bleu_scores = []\n",
    "        for gen_resp in generated_responses[index]:\n",
    "            try:\n",
    "                bleu_scores.append(sentence_bleu([ground_truth[index]], gen_resp, smoothing_function=SmoothingFunction().method7, weights=[1./3, 1./3, 1./3]))\n",
    "            except:\n",
    "                bleu_scores.append(0.0)\n",
    "        \n",
    "        recall_bleus.append(np.max(bleu_scores))\n",
    "        precision_bleus.append(np.mean(bleu_scores))\n",
    "    \n",
    "    \n",
    "    return np.mean(recall_bleus), np.mean(precision_bleus)\n",
    "\n",
    "def count_short_responses(generated_responses):\n",
    "    counts = [0, 0]\n",
    "    for gen_resps in generated_responses:\n",
    "        for gen_resp in gen_resps:\n",
    "            if len(gen_resp) == 0:\n",
    "                counts[0] += 1\n",
    "            elif len(gen_resp) == 1:     \n",
    "                counts[1] += 1\n",
    "    \n",
    "    return counts\n",
    "    \n",
    "    \n",
    "\n",
    "os.chdir('/collection/ka2khan/thesis/Cond_Text_Gen')\n",
    "print(os.getcwd())\n",
    "\n",
    "output_files = {\n",
    "    'VAE-AM-multiturn': 'outputs/multiturn_dialog_vae_gan_mse_multi.txt',\n",
    "}\n",
    "\n",
    "\n",
    "for exp_name, file_path in output_files.items():\n",
    "    f_obj = open(file_path)\n",
    "    generated_responses = []\n",
    "    for line in f_obj:\n",
    "        generated_responses.append(line)\n",
    "\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\n",
    "    config = GPT2Config.from_pretrained('gpt2-large')\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2-large', config=config)\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    \n",
    "    ppl = compute_avg_PPL(model, tokenizer, generated_responses)\n",
    "    \n",
    "    print(f'PPL: {ppl}')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
