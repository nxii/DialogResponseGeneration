{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/collection/ka2khan/thesis/Dialog_Generation/notebooks\n",
      "Reading data file: /collection/ka2khan/thesis/Dialog_Generation/data/DailyDialog/raw/dialogues_train.txt\n",
      "Reading data file: /collection/ka2khan/thesis/Dialog_Generation/data/DailyDialog/raw/dialogues_valid.txt\n",
      "Reading data file: /collection/ka2khan/thesis/Dialog_Generation/data/DailyDialog/raw/dialogues_test.txt\n",
      "Total dialogs in train before deduplication: 10549 after deduplication: 10549\n",
      "Total dialogs in valid before deduplication: 995 after deduplication: 927\n",
      "Total dialogs in test before deduplication: 996 after deduplication: 904\n"
     ]
    }
   ],
   "source": [
    "# DailyDialog Dataset\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import shutil\n",
    "\n",
    "os.chdir('/collection/ka2khan/thesis/Dialog_Generation/notebooks')\n",
    "print(os.getcwd())\n",
    "\n",
    "dataset_path = '/collection/ka2khan/thesis/Dialog_Generation/data/DailyDialog'\n",
    "dataset_parts = ['train', 'valid', 'test']\n",
    "orig_dir = 'original'\n",
    "proc_dir = 'processed'\n",
    "\n",
    "if os.path.isdir(os.path.join(dataset_path, proc_dir)):\n",
    "    shutil.rmtree(os.path.join(dataset_path, proc_dir))\n",
    "\n",
    "\n",
    "data_items = {}\n",
    "for part in dataset_parts:\n",
    "    dir_path = os.path.join(dataset_path, orig_dir)\n",
    "    file_path = os.path.join(dir_path,  f'dialogues_{part}.txt')\n",
    "    print(f'Reading data file: {file_path}')\n",
    "    with open(file_path) as f_obj:\n",
    "        lines = [line.strip() for line in f_obj.readlines() if line.strip() != '']\n",
    "        data_items[part] = list(set(lines))\n",
    "\n",
    "inver_train_items = {item: True for item in data_items['train']}\n",
    "\n",
    "dedup_data_items = {'train': []}\n",
    "\n",
    "for key, val in inver_train_items.items():\n",
    "    dedup_data_items['train'].append(key)\n",
    "\n",
    "for part in dataset_parts:\n",
    "    if part == 'train':\n",
    "        continue\n",
    "\n",
    "    dedup_data_items[part] = []\n",
    "    for item in data_items[part]:\n",
    "        dialog = item\n",
    "        if dialog not in inver_train_items:\n",
    "            dedup_data_items[part].append(dialog)\n",
    "\n",
    "for part in dataset_parts:\n",
    "    print(f'Total dialogs in {part} before deduplication: {len(data_items[part])} after deduplication: {len(dedup_data_items[part])}')\n",
    "\n",
    "utterances = {}\n",
    "contexts = {}\n",
    "queries = {}\n",
    "responses = {}\n",
    "for part in dataset_parts:\n",
    "\n",
    "    utterances[part] = []\n",
    "    contexts[part] = []\n",
    "    queries[part] = []\n",
    "    responses[part] = []\n",
    "    for dialog in dedup_data_items[part]:\n",
    "        dialog_utts = dialog.split('__eou__')\n",
    "\n",
    "        dialog_utts = [utt.strip() for utt in dialog_utts if utt.strip() != '']\n",
    "\n",
    "        utterances[part].extend(dialog_utts)\n",
    "        #print(len(dialog_utts))\n",
    "        for utt_index in range(1, len(dialog_utts)):\n",
    "            #print(utt_index)\n",
    "            #print(dialog_utts[:utt_index])\n",
    "            #print(dialog_utts[utt_index])\n",
    "            if utt_index == 1:\n",
    "                contexts[part].append([])\n",
    "            else:\n",
    "                contexts[part].append(dialog_utts[:utt_index-1])\n",
    "\n",
    "            queries[part].append(dialog_utts[utt_index-1])\n",
    "            responses[part].append(dialog_utts[utt_index])\n",
    "\n",
    "proc_dir_path = os.path.join(dataset_path, proc_dir)\n",
    "os.makedirs(proc_dir_path)\n",
    "\n",
    "\n",
    "utterance_dict = {}    \n",
    "query_resp_dict = {}\n",
    "for part in dataset_parts:            \n",
    "    # Write Utterances File\n",
    "    utterances_file_path = os.path.join(proc_dir_path, f'{part}_utterances.tsv')\n",
    "    with open(utterances_file_path, 'w') as f_obj:\n",
    "        for utterance in utterances[part]:\n",
    "            if utterance in utterance_dict:\n",
    "                continue\n",
    "            else:\n",
    "                utterance_dict[utterance] = True\n",
    "                \n",
    "            f_obj.write(f'{utterance}\\n')\n",
    "\n",
    "    # Write Dialog File\n",
    "    dialog_file_path = os.path.join(proc_dir_path, f'{part}_dialog.tsv')\n",
    "    with open(dialog_file_path, 'w') as f_obj:\n",
    "        for index in range(len(contexts[part])):\n",
    "            context = ' '.join(contexts[part][index])\n",
    "            query = queries[part][index]\n",
    "            response = responses[part][index]\n",
    "            \n",
    "            query_resp_pair = (query, response)\n",
    "            \n",
    "            if query_resp_pair in query_resp_dict:\n",
    "                continue\n",
    "            else:\n",
    "                query_resp_dict[query_resp_pair] = True\n",
    "\n",
    "            f_obj.write(f\"{context}\\t{query}\\t{response}\\n\")\n",
    "        \n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/collection/ka2khan/thesis/Dialog_Generation/notebooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:08<00:00, 11.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train sentences: 30301028\n",
      "Total test sentences: 306688\n",
      "train - 5300000 sentences\n",
      "valid - 300000 sentences\n",
      "test - 306688 sentences\n"
     ]
    }
   ],
   "source": [
    "# 1-Billion Dataset\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import shutil\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "os.chdir('/collection/ka2khan/thesis/Dialog_Generation/notebooks')\n",
    "print(os.getcwd())\n",
    "\n",
    "dataset_path = '/collection/ka2khan/thesis/Dialog_Generation/data/1-Billion'\n",
    "dataset_parts = ['train', 'valid', 'test']\n",
    "orig_dir = 'original'\n",
    "proc_dir = 'processed'\n",
    "\n",
    "if os.path.isdir(os.path.join(dataset_path, proc_dir)):\n",
    "    shutil.rmtree(os.path.join(dataset_path, proc_dir))\n",
    "\n",
    "os.makedirs(os.path.join(dataset_path, proc_dir))\n",
    "\n",
    "    \n",
    "# Read train set files\n",
    "dir_path = os.path.join(dataset_path, orig_dir, 'train')\n",
    "train_file_list = glob.glob(f'{dir_path}/news.en-*')\n",
    "train_file_list.sort()\n",
    "\n",
    "train_sentences = []\n",
    "for file_path in tqdm(train_file_list):\n",
    "    with open(file_path) as f_obj:\n",
    "        train_sentences.extend(f_obj.readlines())\n",
    "    \n",
    "    \n",
    "\n",
    "test_file_path = os.path.join(dataset_path, orig_dir, 'test/news.en-00000-of-00100')\n",
    "with open(test_file_path) as f_obj:\n",
    "    test_sentences = f_obj.readlines()\n",
    "\n",
    "print(f'Total train sentences: {len(train_sentences)}')\n",
    "print(f'Total test sentences: {len(test_sentences)}')\n",
    "\n",
    "indices = np.arange(len(train_sentences))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "valid_indices = indices[:300000]\n",
    "train_indices = indices[300000:5600000]\n",
    "\n",
    "sentences = {'train': [], 'valid': [], 'test': []}\n",
    "sentences['test'] = test_sentences\n",
    "\n",
    "for index in train_indices:\n",
    "    sentences['train'].append(train_sentences[index])\n",
    "\n",
    "for index in valid_indices:\n",
    "    sentences['valid'].append(train_sentences[index])\n",
    "\n",
    "for part in sentences.keys():\n",
    "    print(f'{part} - {len(sentences[part])} sentences')\n",
    "\n",
    "proc_file_paths = {\n",
    "    'train': '/collection/ka2khan/thesis/Dialog_Generation/data/1-Billion/processed/train_sentences.txt',\n",
    "    'valid': '/collection/ka2khan/thesis/Dialog_Generation/data/1-Billion/processed/valid_sentences.txt',\n",
    "    'test': '/collection/ka2khan/thesis/Dialog_Generation/data/1-Billion/processed/test_sentences.txt',\n",
    "}\n",
    "\n",
    "for part, file_path in proc_file_paths.items():\n",
    "    with open(file_path, 'w') as f_obj:\n",
    "        for sentence in sentences[part]:\n",
    "            f_obj.write(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['test', 'train', 'valid'])\n"
     ]
    }
   ],
   "source": [
    "# Switchboard Dataset\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "orig_dataset_path = '/collection/ka2khan/DialogWAE/data/SWDA/full_swda_clean_42da_sentiment_dialog_corpus.p'\n",
    "\n",
    "dataset_path = '/collection/ka2khan/thesis/Cond_Text_Gen/data/SWDA'\n",
    "dataset_parts = ['train', 'valid', 'test']\n",
    "proc_dir = 'processed'\n",
    "\n",
    "with open(orig_dataset_path, 'rb') as f_obj:\n",
    "    orig_dataset = pickle.load(f_obj)\n",
    "\n",
    "print(orig_dataset.keys())\n",
    "\n",
    "parts = ['train', 'valid', 'test']\n",
    "data_items = {}\n",
    "for part in parts:\n",
    "    data_items[part] = []\n",
    "    for dialog in orig_dataset[part]:\n",
    "        utterances = []\n",
    "        last_speaker = None\n",
    "        for utt in dialog['utts']:\n",
    "            utterances.append(utt[1])\n",
    "        data_items[part].append(utterances)\n",
    "        \n",
    "\n",
    "utterances = {}\n",
    "contexts = {}\n",
    "queries = {}\n",
    "responses = {}\n",
    "for part in dataset_parts:\n",
    "\n",
    "    utterances[part] = []\n",
    "    contexts[part] = []\n",
    "    queries[part] = []\n",
    "    responses[part] = []\n",
    "    for dialog_utts in data_items[part]:\n",
    "        dialog_utts = [utt.strip() for utt in dialog_utts if utt.strip() != '']\n",
    "\n",
    "        utterances[part].extend(dialog_utts)\n",
    "        #print(len(dialog_utts))\n",
    "        for utt_index in range(1, len(dialog_utts)):\n",
    "            #print(utt_index)\n",
    "            #print(dialog_utts[:utt_index])\n",
    "            #print(dialog_utts[utt_index])\n",
    "            if utt_index == 1:\n",
    "                contexts[part].append([])\n",
    "            else:\n",
    "                contexts[part].append(dialog_utts[:utt_index-1])\n",
    "\n",
    "            queries[part].append(dialog_utts[utt_index-1])\n",
    "            responses[part].append(dialog_utts[utt_index])\n",
    "\n",
    "proc_dir_path = os.path.join(dataset_path, proc_dir)\n",
    "\n",
    "if os.path.isdir(proc_dir_path):\n",
    "    shutil.rmtree(proc_dir_path)            \n",
    "            \n",
    "os.makedirs(proc_dir_path)\n",
    "\n",
    "\n",
    "utterance_dict = {}    \n",
    "query_resp_dict = {}\n",
    "for part in dataset_parts:            \n",
    "    # Write Utterances File\n",
    "    utterances_file_path = os.path.join(proc_dir_path, f'{part}_sentences.tsv')\n",
    "    with open(utterances_file_path, 'w') as f_obj:\n",
    "        for utterance in utterances[part]:\n",
    "            if utterance in utterance_dict:\n",
    "                continue\n",
    "            else:\n",
    "                utterance_dict[utterance] = True\n",
    "                \n",
    "            f_obj.write(f'{utterance}\\n')\n",
    "\n",
    "    # Write Dialog File\n",
    "    dialog_file_path = os.path.join(proc_dir_path, f'{part}_tuples.tsv')\n",
    "    with open(dialog_file_path, 'w') as f_obj:\n",
    "        for index in range(len(contexts[part])):\n",
    "            context = ' '.join(contexts[part][index])\n",
    "            query = queries[part][index]\n",
    "            response = responses[part][index]\n",
    "            \n",
    "            query_resp_pair = (query, response)\n",
    "            \n",
    "            if query_resp_pair in query_resp_dict:\n",
    "                continue\n",
    "            else:\n",
    "                query_resp_dict[query_resp_pair] = True\n",
    "\n",
    "            f_obj.write(f\"{context}\\t{query}\\t{response}\\n\")\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DailyDialog Dataset\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import shutil\n",
    "\n",
    "os.chdir('/collection/ka2khan/thesis/Dialog_Generation/notebooks')\n",
    "print(os.getcwd())\n",
    "\n",
    "dataset_path = '/collection/ka2khan/thesis/Cond_Text_Gen/data/DailyDialog'\n",
    "dataset_parts = ['train', 'valid', 'test']\n",
    "orig_dir = 'original'\n",
    "proc_dir = 'processed'\n",
    "\n",
    "if os.path.isdir(os.path.join(dataset_path, proc_dir)):\n",
    "    shutil.rmtree(os.path.join(dataset_path, proc_dir))\n",
    "\n",
    "\n",
    "data_items = {}\n",
    "for part in dataset_parts:\n",
    "    dir_path = os.path.join(dataset_path, orig_dir)\n",
    "    file_path = os.path.join(dir_path,  f'dialogues_{part}.txt')\n",
    "    print(f'Reading data file: {file_path}')\n",
    "    with open(file_path) as f_obj:\n",
    "        lines = [line.strip() for line in f_obj.readlines() if line.strip() != '']\n",
    "        data_items[part] = list(set(lines))\n",
    "\n",
    "inver_train_items = {item: True for item in data_items['train']}\n",
    "\n",
    "dedup_data_items = {'train': []}\n",
    "\n",
    "for key, val in inver_train_items.items():\n",
    "    dedup_data_items['train'].append(key)\n",
    "\n",
    "for part in dataset_parts:\n",
    "    if part == 'train':\n",
    "        continue\n",
    "\n",
    "    dedup_data_items[part] = []\n",
    "    for item in data_items[part]:\n",
    "        dialog = item\n",
    "        if dialog not in inver_train_items:\n",
    "            dedup_data_items[part].append(dialog)\n",
    "\n",
    "for part in dataset_parts:\n",
    "    print(f'Total dialogs in {part} before deduplication: {len(data_items[part])} after deduplication: {len(dedup_data_items[part])}')\n",
    "\n",
    "utterances = {}\n",
    "contexts = {}\n",
    "queries = {}\n",
    "responses = {}\n",
    "for part in dataset_parts:\n",
    "\n",
    "    utterances[part] = []\n",
    "    contexts[part] = []\n",
    "    queries[part] = []\n",
    "    responses[part] = []\n",
    "    for dialog in dedup_data_items[part]:\n",
    "        dialog_utts = dialog.split('__eou__')\n",
    "\n",
    "        dialog_utts = [utt.strip() for utt in dialog_utts if utt.strip() != '']\n",
    "\n",
    "        utterances[part].extend(dialog_utts)\n",
    "        #print(len(dialog_utts))\n",
    "        for utt_index in range(1, len(dialog_utts)):\n",
    "            #print(utt_index)\n",
    "            #print(dialog_utts[:utt_index])\n",
    "            #print(dialog_utts[utt_index])\n",
    "            if utt_index == 1:\n",
    "                contexts[part].append([])\n",
    "            else:\n",
    "                contexts[part].append(dialog_utts[:utt_index-1])\n",
    "\n",
    "            queries[part].append(dialog_utts[utt_index-1])\n",
    "            responses[part].append(dialog_utts[utt_index])\n",
    "\n",
    "proc_dir_path = os.path.join(dataset_path, proc_dir)\n",
    "os.makedirs(proc_dir_path)\n",
    "\n",
    "\n",
    "utterance_dict = {}    \n",
    "query_resp_dict = {}\n",
    "for part in dataset_parts:            \n",
    "    # Write Utterances File\n",
    "    utterances_file_path = os.path.join(proc_dir_path, f'{part}_utterances.tsv')\n",
    "    with open(utterances_file_path, 'w') as f_obj:\n",
    "        for utterance in utterances[part]:\n",
    "            if utterance in utterance_dict:\n",
    "                continue\n",
    "            else:\n",
    "                utterance_dict[utterance] = True\n",
    "                \n",
    "            f_obj.write(f'{utterance}\\n')\n",
    "\n",
    "    # Write Dialog File\n",
    "    dialog_file_path = os.path.join(proc_dir_path, f'{part}_dialog.tsv')\n",
    "    with open(dialog_file_path, 'w') as f_obj:\n",
    "        for index in range(len(contexts[part])):\n",
    "            context = ' '.join(contexts[part][index])\n",
    "            query = queries[part][index]\n",
    "            response = responses[part][index]\n",
    "            \n",
    "            query_resp_pair = (query, response)\n",
    "            \n",
    "            if query_resp_pair in query_resp_dict:\n",
    "                continue\n",
    "            else:\n",
    "                query_resp_dict[query_resp_pair] = True\n",
    "\n",
    "            f_obj.write(f\"{context}\\t{query}\\t{response}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/collection/ka2khan/thesis/Cond_Text_Gen/notebooks\n",
      "Reading data file: /collection/ka2khan/thesis/Cond_Text_Gen/data/ROCStories/original/train.csv\n",
      "Reading data file: /collection/ka2khan/thesis/Cond_Text_Gen/data/ROCStories/original/valid.csv\n",
      "Reading data file: /collection/ka2khan/thesis/Cond_Text_Gen/data/ROCStories/original/test.csv\n",
      "\n",
      "Train contains 98161 items.\n",
      "Train contains 1871 items.\n",
      "Train contains 1871 items.\n",
      "\n",
      "train contains:\n",
      "490805 sentences.\n",
      "98161 first sentences.\n",
      "392644 contexts.\n",
      "392644 current sentences.\n",
      "392644 next sentences.\n",
      "\n",
      "valid contains:\n",
      "9355 sentences.\n",
      "1871 first sentences.\n",
      "1871 contexts.\n",
      "1871 current sentences.\n",
      "1871 next sentences.\n",
      "\n",
      "test contains:\n",
      "9355 sentences.\n",
      "1871 first sentences.\n",
      "1871 contexts.\n",
      "1871 current sentences.\n",
      "1871 next sentences.\n"
     ]
    }
   ],
   "source": [
    "# ROCStories Dataset\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import shutil\n",
    "import csv\n",
    "\n",
    "os.chdir('/collection/ka2khan/thesis/Cond_Text_Gen/notebooks')\n",
    "print(os.getcwd())\n",
    "\n",
    "dataset_path = '/collection/ka2khan/thesis/Cond_Text_Gen/data/ROCStories'\n",
    "dataset_parts = ['train', 'valid', 'test']\n",
    "orig_dir = 'original'\n",
    "proc_dir = 'processed'\n",
    "\n",
    "if os.path.isdir(os.path.join(dataset_path, proc_dir)):\n",
    "    shutil.rmtree(os.path.join(dataset_path, proc_dir))\n",
    "\n",
    "os.makedirs(os.path.join(dataset_path, proc_dir))\n",
    "\n",
    "data_items = {}\n",
    "for part in dataset_parts:\n",
    "    dir_path = os.path.join(dataset_path, orig_dir)\n",
    "    file_path = os.path.join(dir_path,  f'{part}.csv')\n",
    "    print(f'Reading data file: {file_path}')\n",
    "    data_items[part] = []\n",
    "    with open(file_path) as f_obj:\n",
    "        reader = csv.reader(f_obj)\n",
    "        for index, row in enumerate(reader):\n",
    "            if index ==0:\n",
    "                continue\n",
    "\n",
    "            data_items[part].append(row)    \n",
    "\n",
    "print()\n",
    "for part in dataset_parts:\n",
    "    print(f'Train contains {len(data_items[part])} items.')\n",
    "    \n",
    "sentences = {}\n",
    "first_sents = {}\n",
    "contexts = {}\n",
    "curr_sents = {}\n",
    "next_sents = {}\n",
    "for part in dataset_parts:\n",
    "    sentences[part] = []\n",
    "    contexts[part] = []\n",
    "    first_sents[part] = []\n",
    "    curr_sents[part] = []\n",
    "    next_sents[part] = []\n",
    "    \n",
    "    for item in data_items[part]:\n",
    "        if part == 'train':\n",
    "            assert len(item) == 7, f'Error! train item does not contain exactly 7 parts! {item}'\n",
    "            \n",
    "            story_sents = item[2:]\n",
    "            \n",
    "            story_sents = [sent.strip() for sent in story_sents]\n",
    "            \n",
    "            sentences[part].extend(story_sents)\n",
    "            \n",
    "            first_sents[part].append(story_sents[0])\n",
    "            \n",
    "            for sent_index in range(1, len(story_sents)):\n",
    "                if sent_index == 1:\n",
    "                    contexts[part].append([])\n",
    "                else:                \n",
    "                    contexts[part].append(story_sents[:sent_index-1])\n",
    "\n",
    "                curr_sents[part].append(story_sents[sent_index-1])\n",
    "                next_sents[part].append(story_sents[sent_index])\n",
    "        else:        \n",
    "            assert len(item) == 8, f'Error! {part} item does not contain exactly 8 parts! {item}'\n",
    "            \n",
    "            story_sents = item[1:5]\n",
    "            \n",
    "            story_sents = [sent.strip() for sent in story_sents]\n",
    "            \n",
    "            con_alts = item[5:7]\n",
    "            true_ending = int(item[-1])\n",
    "            story_sents.append(con_alts[true_ending-1])\n",
    "            \n",
    "            sentences[part].extend(story_sents)\n",
    "            \n",
    "            first_sents[part].append(story_sents[0])\n",
    "            \n",
    "            contexts[part].append(story_sents[:len(story_sents)-2])\n",
    "                \n",
    "            curr_sents[part].append(story_sents[len(story_sents)-2])\n",
    "            next_sents[part].append(story_sents[len(story_sents)-1])\n",
    "        \n",
    "        \n",
    "            \n",
    "for part in dataset_parts:\n",
    "    print()\n",
    "    print(f'{part} contains:')\n",
    "    print(f'{len(sentences[part])} sentences.')\n",
    "    print(f'{len(first_sents[part])} first sentences.')\n",
    "    print(f'{len(contexts[part])} contexts.')\n",
    "    print(f'{len(curr_sents[part])} current sentences.')\n",
    "    print(f'{len(next_sents[part])} next sentences.')\n",
    "    \n",
    "    file_path = os.path.join(dataset_path, proc_dir, f'{part}_sentences.tsv')\n",
    "    with open(file_path, 'w') as f_obj:\n",
    "        for sent in sentences[part]:\n",
    "            f_obj.write(f'{sent}\\n')\n",
    "    \n",
    "    file_path = os.path.join(dataset_path, proc_dir, f'{part}_first_sents.tsv')\n",
    "    with open(file_path, 'w') as f_obj:\n",
    "        for sent in first_sents[part]:\n",
    "            f_obj.write(f'{sent}\\n')\n",
    "            \n",
    "    file_path = os.path.join(dataset_path, proc_dir, f'{part}_tuples.tsv')\n",
    "    with open(file_path, 'w') as f_obj:\n",
    "        for index in range(len(contexts[part])):\n",
    "            f_obj.write(f\"{' '.join(contexts[part][index])}\\t{curr_sents[part][index]}\\t{next_sents[part][index]}\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/collection/ka2khan/thesis/Cond_Text_Gen/notebooks\n",
      "\n",
      "Processing data file: /collection/ka2khan/thesis/Cond_Text_Gen/data/Taskmaster-2/original/movies.json\n",
      "Dialogs found: 3056, Utterances found: 51129\n",
      "Training set size: 2444\n",
      "Validation set size: 306\n",
      "Test set size: 306\n",
      "\n",
      "Processing data file: /collection/ka2khan/thesis/Cond_Text_Gen/data/Taskmaster-2/original/sports.json\n",
      "Dialogs found: 3481, Utterances found: 44796\n",
      "Training set size: 2783\n",
      "Validation set size: 349\n",
      "Test set size: 349\n",
      "\n",
      "Processing data file: /collection/ka2khan/thesis/Cond_Text_Gen/data/Taskmaster-2/original/hotels.json\n",
      "Dialogs found: 2357, Utterances found: 52700\n",
      "Training set size: 1885\n",
      "Validation set size: 236\n",
      "Test set size: 236\n",
      "\n",
      "Processing data file: /collection/ka2khan/thesis/Cond_Text_Gen/data/Taskmaster-2/original/music.json\n",
      "Dialogs found: 1603, Utterances found: 21358\n",
      "Training set size: 1281\n",
      "Validation set size: 161\n",
      "Test set size: 161\n",
      "\n",
      "Processing data file: /collection/ka2khan/thesis/Cond_Text_Gen/data/Taskmaster-2/original/restaurant-search.json\n",
      "Dialogs found: 3276, Utterances found: 55777\n",
      "Training set size: 2620\n",
      "Validation set size: 328\n",
      "Test set size: 328\n",
      "\n",
      "Processing data file: /collection/ka2khan/thesis/Cond_Text_Gen/data/Taskmaster-2/original/food-ordering.json\n",
      "Dialogs found: 1050, Utterances found: 12290\n",
      "Training set size: 840\n",
      "Validation set size: 105\n",
      "Test set size: 105\n",
      "\n",
      "Processing data file: /collection/ka2khan/thesis/Cond_Text_Gen/data/Taskmaster-2/original/flights.json\n",
      "Dialogs found: 2481, Utterances found: 55068\n",
      "Training set size: 1983\n",
      "Validation set size: 249\n",
      "Test set size: 249\n",
      "Total Dialogs found: 17304, Utterances found: 293118\n",
      "\n",
      "Training set contains 13836 dialogs.\n",
      "Validation set contains 1734 dialogs.\n",
      "Test set contains 1734 dialogs.\n"
     ]
    }
   ],
   "source": [
    "# Taskmaster-2 dataset\n",
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import shutil\n",
    "import csv\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "os.chdir('/collection/ka2khan/thesis/Cond_Text_Gen/notebooks')\n",
    "print(os.getcwd())\n",
    "\n",
    "dataset_path = '/collection/ka2khan/thesis/Cond_Text_Gen/data/Taskmaster-2'\n",
    "dataset_parts = ['train', 'valid', 'test']\n",
    "orig_dir = 'original'\n",
    "proc_dir = 'processed'\n",
    "\n",
    "if os.path.isdir(os.path.join(dataset_path, proc_dir)):\n",
    "    shutil.rmtree(os.path.join(dataset_path, proc_dir))\n",
    "\n",
    "os.makedirs(os.path.join(dataset_path, proc_dir))\n",
    "\n",
    "original_path = os.path.join(dataset_path, orig_dir)\n",
    "\n",
    "data_files = glob.glob(f'{original_path}/*.json')\n",
    "\n",
    "\n",
    "train = []\n",
    "valid = []\n",
    "test = []\n",
    "utt_counts = {}\n",
    "diag_counts = {}\n",
    "for data_file in data_files:\n",
    "    print()\n",
    "    print(f'Processing data file: {data_file}')\n",
    "    with open(data_file) as f_obj:\n",
    "        data = json.load(f_obj)\n",
    "        \n",
    "    dialogs = []\n",
    "    utt_counts[data_file] = 0\n",
    "    for item in data:\n",
    "        utterances = []\n",
    "        prev_speaker = None\n",
    "        for utt_details in item['utterances']:\n",
    "            if utt_details['speaker'].lower() == prev_speaker:\n",
    "                utterances[-1] = utterances[-1] + ' ' + utt_details['text']\n",
    "            else:\n",
    "                utterances.append(utt_details['text'])\n",
    "                prev_speaker = utt_details['speaker'].lower()\n",
    "            \n",
    "        \n",
    "        utt_counts[data_file] += len(utterances)\n",
    "        dialogs.append(utterances)\n",
    "    \n",
    "    diag_counts[data_file] = len(dialogs)\n",
    "    \n",
    "    print(f'Dialogs found: {diag_counts[data_file]}, Utterances found: {utt_counts[data_file]}')\n",
    "    \n",
    "    indices = np.arange(len(dialogs))\n",
    "    \n",
    "    #randomly shuffle indices\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    test_indices = indices[:math.ceil(len(indices)*0.1)]\n",
    "    valid_indices = indices[math.ceil(len(indices)*0.1):2*math.ceil(len(indices)*0.1)]\n",
    "    train_indices = indices[2*math.ceil(len(indices)*0.1):]\n",
    "    \n",
    "    print(f'Training set size: {len(train_indices)}')\n",
    "    print(f'Validation set size: {len(valid_indices)}')\n",
    "    print(f'Test set size: {len(test_indices)}')\n",
    "    \n",
    "    for index in train_indices:\n",
    "        train.append(dialogs[index])\n",
    "    \n",
    "    for index in valid_indices:\n",
    "        valid.append(dialogs[index])\n",
    "    \n",
    "    for index in test_indices:\n",
    "        test.append(dialogs[index])\n",
    "        \n",
    "    \n",
    "print(f'Total Dialogs found: {np.sum(list(diag_counts.values()))}, Utterances found: {np.sum(list(utt_counts.values()))}')\n",
    "\n",
    "print()\n",
    "print(f'Training set contains {len(train)} dialogs.')\n",
    "print(f'Validation set contains {len(valid)} dialogs.')\n",
    "print(f'Test set contains {len(test)} dialogs.')\n",
    "\n",
    "data = {'train': train, 'valid': valid, 'test': test}\n",
    "\n",
    "utterances = {}\n",
    "contexts = {}\n",
    "queries = {}\n",
    "responses = {}\n",
    "for part in dataset_parts:\n",
    "\n",
    "    utterances[part] = []\n",
    "    contexts[part] = []\n",
    "    queries[part] = []\n",
    "    responses[part] = []\n",
    "    for dialog_utts in data[part]:\n",
    "        \n",
    "        dialog_utts = [utt.strip() for utt in dialog_utts if utt.strip() != '']\n",
    "\n",
    "        utterances[part].extend(dialog_utts)\n",
    "        #print(len(dialog_utts))\n",
    "        for utt_index in range(1, len(dialog_utts)):\n",
    "            #print(utt_index)\n",
    "            #print(dialog_utts[:utt_index])\n",
    "            #print(dialog_utts[utt_index])\n",
    "            if utt_index == 1:\n",
    "                contexts[part].append([])\n",
    "            else:\n",
    "                contexts[part].append(dialog_utts[:utt_index-1])\n",
    "\n",
    "            queries[part].append(dialog_utts[utt_index-1])\n",
    "            responses[part].append(dialog_utts[utt_index])\n",
    "\n",
    "proc_dir_path = os.path.join(dataset_path, proc_dir)\n",
    "            \n",
    "utterance_dict = {}    \n",
    "query_resp_dict = {}\n",
    "for part in dataset_parts:            \n",
    "    # Write Utterances File\n",
    "    utterances_file_path = os.path.join(proc_dir_path, f'{part}_sentences.tsv')\n",
    "    with open(utterances_file_path, 'w') as f_obj:\n",
    "        for utterance in utterances[part]:\n",
    "            if utterance in utterance_dict:\n",
    "                continue\n",
    "            else:\n",
    "                utterance_dict[utterance] = True\n",
    "                \n",
    "            f_obj.write(f'{utterance}\\n')\n",
    "\n",
    "    # Write Dialog File\n",
    "    dialog_file_path = os.path.join(proc_dir_path, f'{part}_tuples.tsv')\n",
    "    with open(dialog_file_path, 'w') as f_obj:\n",
    "        for index in range(len(contexts[part])):\n",
    "            context = ' '.join(contexts[part][index])\n",
    "            query = queries[part][index]\n",
    "            response = responses[part][index]\n",
    "            \n",
    "            query_resp_pair = (query, response)\n",
    "            \n",
    "            if query_resp_pair in query_resp_dict:\n",
    "                continue\n",
    "            else:\n",
    "                query_resp_dict[query_resp_pair] = True\n",
    "\n",
    "            f_obj.write(f\"{context}\\t{query}\\t{response}\\n\")\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/collection/ka2khan/thesis/Cond_Text_Gen/notebooks\n",
      "\n",
      "Processing data file: /collection/ka2khan/thesis/Cond_Text_Gen/data/Taskmaster-2/original/movies.json\n",
      "\n",
      "Processing data file: /collection/ka2khan/thesis/Cond_Text_Gen/data/Taskmaster-2/original/sports.json\n",
      "\n",
      "Processing data file: /collection/ka2khan/thesis/Cond_Text_Gen/data/Taskmaster-2/original/hotels.json\n",
      "\n",
      "Processing data file: /collection/ka2khan/thesis/Cond_Text_Gen/data/Taskmaster-2/original/music.json\n",
      "\n",
      "Processing data file: /collection/ka2khan/thesis/Cond_Text_Gen/data/Taskmaster-2/original/restaurant-search.json\n",
      "\n",
      "Processing data file: /collection/ka2khan/thesis/Cond_Text_Gen/data/Taskmaster-2/original/food-ordering.json\n",
      "\n",
      "Processing data file: /collection/ka2khan/thesis/Cond_Text_Gen/data/Taskmaster-2/original/flights.json\n",
      "Total Dialogs found: 5794\n",
      "train contains 12865 dialogs\n",
      "valid contains 1574 dialogs\n",
      "test contains 1577 dialogs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import shutil\n",
    "import csv\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "\n",
    "os.chdir('/collection/ka2khan/thesis/Cond_Text_Gen/notebooks')\n",
    "print(os.getcwd())\n",
    "\n",
    "dataset_path = '/collection/ka2khan/thesis/Cond_Text_Gen/data/Taskmaster-2'\n",
    "dataset_parts = ['train', 'valid', 'test']\n",
    "orig_dir = 'original'\n",
    "proc_dir = 'processed'\n",
    "\n",
    "dest_path = '/collection/ka2khan/DialogWAE/data/Taskmaster2'\n",
    "\n",
    "original_path = os.path.join(dataset_path, orig_dir)\n",
    "\n",
    "data_files = glob.glob(f'{original_path}/*.json')\n",
    "\n",
    "dialogs = {}\n",
    "for data_file in data_files:\n",
    "    print()\n",
    "    print(f'Processing data file: {data_file}')\n",
    "    with open(data_file) as f_obj:\n",
    "        data = json.load(f_obj)\n",
    "        \n",
    "    for item in data:\n",
    "        utterances = []\n",
    "        prev_speaker = None\n",
    "        for utt_details in item['utterances']:\n",
    "            if utt_details['speaker'].lower() == prev_speaker:\n",
    "                utterances[-1] = utterances[-1] + ' ' + (utt_details['text'].strip())\n",
    "            else:\n",
    "                utterances.append(utt_details['text'].strip())\n",
    "                prev_speaker = utt_details['speaker'].lower()\n",
    "            \n",
    "        \n",
    "        dialogs[utterances[0]] = utterances\n",
    "    \n",
    "\n",
    "print(f'Total Dialogs found: {len(dialogs)}')\n",
    "\n",
    "data_items = {}\n",
    "for part in dataset_parts:\n",
    "    data_items[part] = []\n",
    "    file_path = os.path.join(dataset_path, proc_dir, f'{part}_tuples.tsv')\n",
    "    f_obj = open(file_path)\n",
    "    reader = csv.reader(f_obj, delimiter='\\t')\n",
    "    \n",
    "    for row in reader:\n",
    "        \n",
    "        if row[0].strip() != '':\n",
    "            continue\n",
    "        \n",
    "        data_items[part].append(dialogs[row[1]])\n",
    "\n",
    "    print(f'{part} contains {len(data_items[part])} dialogs')\n",
    "    \n",
    "    f_obj = open(os.path.join(dest_path, f'{part}.utts.txt'), 'w')\n",
    "    for dialog in data_items[part]:\n",
    "        line = ' __eou__ '.join(dialog)\n",
    "        f_obj.write(f'{line}\\n')\n",
    "    f_obj.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
